{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nPivotal Cloud Cache (PCC) is a high-performance, high-availability caching layer for Pivotal Cloud Foundry (PCF). \nPCC offers an in-memory key-value store. It delivers low-latency responses to a large number of concurrent data access requests.\n\n\nPCC provides a service broker to create in-memory data clusters on demand. \nThese clusters are dedicated to the PCF space and tuned for specific use cases defined by your service plan. \nService operators can create multiple plans to support different use cases.\n\n\nPCC uses Pivotal GemFire. The \nPivotal GemFire API Documentation\n details the API for client access to data objects within Pivotal GemFire.\n\n\nThis documentation performs the following functions:\n\n\n\n\nDescribes the features and architecture of PCC\n\n\nProvides the PCF operator with instructions for installing, configuring, and maintaining PCC\n\n\nProvides app developers instructions for choosing a service plan, creating, and deleting PCC service instances\n\n\nProvides app developers instructions for binding apps\n\n\n\n\nProduct Snapshot\n\n\nThe following table provides version and version-support information about PCC:\n\n\n\n    \nElement\n\n    \nDetails\n\n    \n\n        \nVersion\n\n        \nv1.4.0\n\n    \n\n    \n\n        \nRelease date\n\n        \nMay 14, 2018\n\n    \n\n    \n\n        \nSoftware component version\n\n        \nGemFire v9.3.0\n\n    \n\n    \n\n        \nCompatible Ops Manager version(s)\n\n        \nv2.1.x and v2.0.x\n\n    \n\n    \n\n        \nCompatible Pivotal Application Service (PAS)* version(s)\n\n        \nv2.1.x and v2.0.x\n\n    \n\n    \n\n        \nIaaS support\n\n        \nAWS, Azure, GCP, OpenStack, and vSphere\n\n    \n\n    \n\n        \nIPsec support\n\n        \nYes\n\n    \n\n    \n\n        \nRequired BOSH stemcell version\n\n        \n3541\n\n    \n\n    \n\n        \nMinimum Java buildpack version required for apps\n\n        \nv3.13\n\n    \n\n\n\n\n\n* As of PCF v2.0, \nElastic Runtime\n is renamed \nPivotal Application Service (PAS)\n.\n\n\n PCC and Other PCF Services\n\n\n<\n%= partial '../../p-cloud-cache/odb/on-demand-service-table' %>\n\n\nPCC Architecture\n\n\nGemFire Basics\n\n\nPivotal GemFire is the data store within Pivotal Cloud Cache (PCC). A small amount of administrative GemFire setup is required for a PCC service instance, and any app will use a limited portion of the GemFire API.\n\n\nThe PCC architectural model is a client-server model. The clients are apps or microservices, and the servers are a set of GemFire servers maintained by a PCC service instance. The GemFire servers provide a low-latency, consistent, fault-tolerant data store within PCC.\n\n\n\n\nGemFire holds data in key/value pairs. Each pair is called an \nentry\n. Entries are logically grouped into sets called regions. A region is a map (or dictionary) data structure.\n\n\nThe app (client) uses PCC as a cache. A cache lookup (read) is a get operation on a GemFire region. The cache operation of a cache write is a put operation on a GemFire region.\nThe GemFire command-line interface, called \ngfsh\n, facilitates region administration. Use \ngfsh\n to create and destroy regions within the PCC service instance.\n\n\nThe PCC Cluster\n\n\nPCC deploys cache clusters that use Pivotal GemFire to provide high availability, replication guarantees, and eventual consistency.\n\n\nWhen you first spin up a cluster, you have three locators and at least four servers.\n\n\n<\n% mermaid_diagram do %>\n  graph TD;\n  Client\n  subgraph P-CloudCache Cluster\n  subgraph locators\n  Locator1\n  Locator2\n  Locator3\n  end\n  subgraph servers\n  Server1\n  Server2\n  Server3\n  Server4\n  end\n  end\n  Client==>Locator1\n  Client-->Server1\n  Client-->Server2\n  Client-->Server3\n  Client-->Server4\n\n<\n% end %>\n\n\nWhen you scale the cluster up, you have more servers, increasing the capacity of the cache. There are always three locators.\n\n\n<\n% mermaid_diagram do %>\n  graph TD;\n  Client\n  subgraph P-CloudCache Cluster\n  subgraph locators\n  Locator1\n  Locator2\n  Locator3\n  end\n  subgraph servers\n  Server1\n  Server2\n  Server3\n  Server4\n  Server5\n  Server6\n  Server7\n  end\n  end\n  Client==>Locator1\n  Client-->Server1\n  Client-->Server2\n  Client-->Server3\n  Client-->Server4\n  Client-->Server5\n  Client-->Server6\n  Client-->Server7\n\n<\n% end %>\n\n\nMember Communication\n\n\nWhen a client connects to the cluster, it first connects to a locator. The locator replies with the IP address of a server for it to talk to. The client then connects to that server.\n\n\n<\n% mermaid_diagram do %>\n  sequenceDiagram\n    participant Client\n    participant Locator\n    participant Server1\n    Client->>+Locator: What servers can I talk to?\n    Locator->>-Client: Server1\n    Client->>Server1: Hello!\n\n<\n% end %>\n\n\nWhen the client wants to read or write data, it sends a request directly to the server.\n\n\n<\n% mermaid_diagram do %>\n  sequenceDiagram\n    participant Client\n    participant Server1\n    Client->>+Server1: What's the value for KEY?\n    Server1->>-Client: VALUE\n\n<\n% end %>\n\n\nIf the server doesn't have the data locally, it fetches it from another server.\n\n\n<\n% mermaid_diagram do %>\n  sequenceDiagram\n    participant Client\n    participant Server1\n    participant Server2\n    Client->>+Server1: What's the value for KEY?\n    Server1->>+Server2: What's the value for KEY?\n    Server2->>-Server1: VALUE\n    Server1->>-Client: VALUE\n\n<\n% end %>\n\n\nWorkflow to Set Up a PCC Service\n\n\nThe workflow for the PCF admin setting up a PCC service plan:\n\n\n<\n% mermaid_diagram do %>\n  graph TD;\n  subgraph PCF Admin Actions\n  s1\n  s2\n  end\n\n\nsubgraph Developer Actions\n  s4\n  end\n\n\ns1[1. Upload P-CloudCache.pivotal to Ops Manager]\n  s2[2. Configure CloudCache Service Plans, i.e. caching-small]\n  s1\ns2\n  s3[3. Ops Manager deploys CloudCache Service Broker]\n  s2\ns3\n  s4[4. Developer calls \ncf create-service p-cloudcache caching-small test\n]\n  s3\ns4\n  s5[5. Ops Manager creates a CloudCache cluster following the caching-small specifications]\n  s4\ns5\n\n% end %\n\n\nNetworking for On-Demand Services\n\n\nThis section describes networking considerations for Pivotal Cloud Cache.\n\n\nBOSH 2.0 and the Service Network\n\n\n<\n%= partial '../../p-cloud-cache/odb/service_networks' %>\n\n\nDefault Network and Service Network\n\n\nLike other on-demand PCF services, PCC relies on the BOSH 2.0 ability to dynamically deploy VMs in a dedicated network. The on-demand service broker uses this capability to create single-tenant service instances in a dedicated service network.\n\n\n<\n%= partial '../../p-cloud-cache/odb/on_demand_architecture' %>\n\n\nThe diagram below shows worker VMs in an on-demand service instance, such as RabbitMQ for PCF, running on a separate services network, while other components run on the default network.\n\n\n\n\nRequired Networking Rules for On-Demand Services\n\n\n<\n%= partial '../../p-cloud-cache/odb/service_networks_table' %>\n\n\n\nRegardless of the specific network layout, the operator must ensure network \nrules are set up so that connections are open as described in the table below. \n\n\n\n  \nThis component...\n\n  \nMust communicate with...\n\n  \nDefault TCP Port\n\n  \nCommunication direction(s)\n\n  \nNotes\n\n  \n\n    \nODB\n\n    \n\n        \n\n            \nBOSH Director\n\n          \nBOSH UAA\n\n        \n\n    \n\n    \n\n      \n\n        \n25555\n\n        \n8443\n\n      \n\n    \n\n    \nOne-way\n\n    \nThe default ports are not configurable.\n\n  \n\n  \n\n    \nODB\n\n    \nDeployed service instances\n\n    \n\n    \nSpecific to the service (such as RabbitMQ for PCF). \n      May be one or more ports.\n\n    \nOne-way\n\n    \nThis connection is for administrative tasks. \n      Avoid opening general use, app-specific ports for this connection.\n\n  \n\n  \n\n    \nODB\n\n    \nPAS (or Elastic Runtime) \n\n    \n\n    \n8443\n\n    \nOne-way\n\n    \nThe default port is not configurable.\n\n  \n\n  \n\n    \nErrand VMs\n\n    \n\n      \n\n        \nPAS (or Elastic Runtime) \n\n        \nODB\n\n        \nDeployed Service Instances\n\n      \n\n    \n\n    \n\n      \n\n        \n8443\n\n        \n8080\n\n        \nSpecific to the service. May be one or more ports.\n\n      \n\n    \n\n    \nOne-way\n\n    \nThe default port is not configurable.\n\n  \n\n  \n\n    \nBOSH Agent\n\n    \nBOSH Director\n\n    \n\n    \n4222\n\n    \nTwo-way\n\n    \nThe BOSH Agent runs on every VM in the system, including the BOSH Director VM. \n      The BOSH Agent initiates the connection with the BOSH Director.\n\n      The default port is not configurable.  \n\n  \n\n  \n\n    \nDeployed apps on PAS (or Elastic Runtime) \n\n    \nDeployed service instances\n\n    \n\n    \nSpecific to the service. May be one or more ports.\n\n    \nOne-way\n\n    \nThis connection is for general use, app-specific tasks. \n      Avoid opening administrative ports for this connection.\n\n  \n\n  \n\n    \nPAS (or Elastic Runtime) \n\n    \nODB\n\n    \n\n    \n8080\n\n    \nOne-way\n\n    \nThis port may be different for individual services. \n      This port may also be configurable by the operator if allowed by the \n      tile developer.\n\n  \n\n\n\n\n\nPCC Instances Across WAN\n\n\nPCC service instances running within distinct PCF foundations\nmay communicate with each other across a WAN.\nIn a topology such as this,\nthe members within one service instance use their own private address space,\nas defined in \nRFC1918\n.\n\n\nA VPN may be used to connect the private network spaces that lay\nacross the WAN.\nThe steps required to enable the connectivity by VPN are dependent\non the IaaS provider(s).\n\n\nThe private address space for each service instance's network\nmust be configured with non-overlapping CIDR blocks.\nConfigure the network prior to creating service instances.\nLocate directions for creating a network on the appropriate IAAS provider\nwithin the section titled\n\nArchitecture and Installation Overview\n.\n\n\nRecommended Usage and Limitations\n\n\n\n\nPCC supports the \nlook-aside cache pattern\n.\n\n\nPCC stores objects in key/value format, where value can be any object.\n\n\nAny gfsh command not explained in the PCC documentation is \nnot supported\n.\n\n\nPCC supports basic OQL queries, with no support for joins.\n\n\n\n\nLimitations\n\n\n\n\nScale down of the cluster is not supported.\n\n\nPlan migrations, for example, \n-p\n flag with the \ncf update-service\n command, are not supported.\n\n\n\n\nSecurity\n\n\nPivotal recommends that you do the following:\n\n\n\n\nRun PCC in its own network\n\n\nUse a load balancer to block direct, outside access to the Gorouter\n\n\n\n\nTo allow PCC network access from apps, you must create application security groups that allow access on the following ports:\n\n\n\n\n1099\n\n\n8080\n\n\n40404\n\n\n55221\n\n\n\n\nFor more information, see the PCF \nApplication Security Groups\n topic.\n\n\nPCC works with the IPsec Add-on for PCF.\nFor information about the IPsec Add-on for PCF,\nsee \nSecuring Data in Transit with the IPsec Add-on\n.\n\n\nAuthentication\n\n\nPCC service instances are created with three default GemFire user roles for\ninteracting with clusters:\n\n\n\n\nA cluster operator manages the GemFire cluster and can access\nregion data.\n\n\nA developer can access region data.\n\n\nA gateway sender propagates region data to another PCC service instance.\n\n\n\n\nAll client apps, gfsh, and JMX clients must authenticate as\none of these user roles to access the cluster.\n\n\nThe identifiers assigned for these roles are detailed in\n\nCreate Service Keys\n.\n\n\nAuthorization\n\n\nEach user role is given predefined permissions for cluster operations.\nTo accomplish a cluster operation,\nthe user authenticates using one of the roles.\nPrior to initiating the requested operation,\nthere is a verification that the\nauthenticated user role has the permission authorized to do the operation.\nHere are the permissions that each user role has:\n\n\n\n\nThe  cluster operator role has \n\nCLUSTER\n:\nMANAGE\n,\n\nCLUSTER\n:\nWRITE\n,\n\nCLUSTER\n:\nREAD\n,\n\nDATA\n:\nMANAGE\n,\n\nDATA\n:\nWRITE\n,\n and \nDATA\n:\nREAD\n permissions.\n\n\nThe developer role has\n\nCLUSTER\n:\nREAD\n,\n\nDATA\n:\nWRITE\n,\nand \nDATA\n:\nREAD\n permissions.\n\n\nThe gateway sender role has \nDATA\n:\nWRITE\n permission.\n\n\n\n\nMore details about these permissions are in the Pivotal GemFire manual under \nImplementing Authorization\n.\n\n\n Feedback\n\n\nPlease provide any bugs, feature requests, or questions to the \nPivotal Cloud Foundry Feedback list\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#limitations", 
            "text": "Scale down of the cluster is not supported.  Plan migrations, for example,  -p  flag with the  cf update-service  command, are not supported.", 
            "title": "Limitations"
        }, 
        {
            "location": "/#authentication", 
            "text": "PCC service instances are created with three default GemFire user roles for\ninteracting with clusters:   A cluster operator manages the GemFire cluster and can access\nregion data.  A developer can access region data.  A gateway sender propagates region data to another PCC service instance.   All client apps, gfsh, and JMX clients must authenticate as\none of these user roles to access the cluster.  The identifiers assigned for these roles are detailed in Create Service Keys .", 
            "title": "Authentication"
        }, 
        {
            "location": "/#authorization", 
            "text": "Each user role is given predefined permissions for cluster operations.\nTo accomplish a cluster operation,\nthe user authenticates using one of the roles.\nPrior to initiating the requested operation,\nthere is a verification that the\nauthenticated user role has the permission authorized to do the operation.\nHere are the permissions that each user role has:   The  cluster operator role has  CLUSTER : MANAGE , CLUSTER : WRITE , CLUSTER : READ , DATA : MANAGE , DATA : WRITE ,\n and  DATA : READ  permissions.  The developer role has CLUSTER : READ , DATA : WRITE ,\nand  DATA : READ  permissions.  The gateway sender role has  DATA : WRITE  permission.   More details about these permissions are in the Pivotal GemFire manual under  Implementing Authorization .", 
            "title": "Authorization"
        }, 
        {
            "location": "/app-development/", 
            "text": "In this topic:\n\n\n\n\nDesign Patterns\n\n\nThe Inline Cache\n\n\nThe Look-Aside Cache\n\n\nBidirectional Replication Across a WAN\n\n\nBlue-Green Disaster Recovery\n\n\nCQRS Pattern Across a WAN\n\n\nHub-and-Spoke Topology with WAN Replication\n\n\nFollow-the-Sun Pattern\n\n\n\n\n\n\nRegion Design\n\n\nKeys\n\n\nPartitioned Regions\n\n\nReplicated Regions\n\n\nPersistence\n\n\nRegions as Used by the App\n\n\nAn Example to Demonstrate Region Design\n\n\n\n\n\n\nUse a Sample Java Client App with PCC", 
            "title": "Application Development"
        }, 
        {
            "location": "/developer/", 
            "text": "This document describes how a Pivotal Cloud Foundry (PCF) app developer can choose a service plan, create and delete Pivotal Cloud Cache (PCC) service instances, and bind an app.\n\n\nYou must install the \nCloud Foundry Command Line Interface\n (cf CLI) to run the commands in this topic.\n\n\nIn this topic:\n\n\n\n\nViewing All Plans Available for Pivotal Cloud Cache\n\n\nCreating a Pivotal Cloud Cache Service Instance\n\n\nProvide Optional Parameters\n\n\nEnable Session State Caching with the Java Buildpack\n\n\nEnable Session State Caching Using Spring Session\n\n\nDev Plans\n\n\n\n\n\n\nSet Up WAN-Separated Service Instances\n\n\nSet Up a Bidirectional System\n\n\nSet Up a Unidirectional System\n\n\n\n\n\n\nDeleting a Service Instance\n\n\nUpdating a Pivotal Cloud Cache Service Instance\n\n\nRebalancing a Cluster\n\n\nRestarting a Cluster\n\n\nAbout Changes to the Service Plan\n\n\n\n\n\n\nAccessing a Service Instance\n\n\nCreate Service Keys\n\n\nConnect with gfsh over HTTPS\n\n\nCreate a Truststore\n\n\nEstablish the Connection with HTTPS\n\n\nEstablish the Connection with HTTPS  in a Development Environment\n\n\n\n\n\n\n\n\n\n\nUsing Pivotal Cloud Cache\n\n\nCreate Regions with gfsh\n\n\nJava Build Pack Requirements\n\n\nBind an App to a Service Instance\n\n\nUse the Pulse Dashboard\n\n\nAccess Service Metrics\n\n\nAccess Service Broker Metrics\n\n\nExport gfsh logs\n\n\nDeploy an App JAR File to the Servers\n\n\n\n\n\n\nConnecting a Spring Boot App to Pivotal Cloud Cache with Session State Caching\n\n\nUse the Tomcat App\n\n\nUse a Spring Session Data GemFire App\n\n\n\n\n\n\nCreating Continuous Queries Using Spring Data GemFire", 
            "title": "Developer"
        }, 
        {
            "location": "/operator/", 
            "text": "This document describes how a Pivotal Cloud Foundry (PCF) operator can install, configure, and maintain Pivotal Cloud Cache (PCC).\n\n\n Requirements for Pivotal Cloud Cache\n\n\nService Network\n\n\nYou must have access to a Service Network in order to install PCC. \n\n\n Installing and Configuring Pivotal Cloud Cache\n\n\nWith an Ops Manager role (detailed in \nUnderstand Roles in Ops Manager\n)\nthat has the proper permissions to install and configure,\nfollow these steps to install PCC on PCF:\n\n\n\n\nDownload the tile from the \nPivotal Network\n.\n\n\nClick \nImport a Product\n to import the tile into Ops Manager.\n\n\nClick the \n+\n symbol next to the uploaded product description.\n\n\nClick on the Cloud Cache tile.\n\n\nComplete all the configuration steps in the \nConfigure Tile Properties\n section below.\n\n\nReturn to the Ops Manager Installation Dashboard and click \nApply Changes\n to complete the installation of the PCC tile.  \n\n\n\n\n Configure Tile Properties\n\n\nConfigure the sections listed on the left side of the page. \n\n\n\n\nAfter you complete a section, a green check mark appears next to the section name.\nEach section name must show this green check mark before you can complete your installation.\n\n\n\n\nAssign AZs and Networks\n\n\nSettings\n\n\nService Plans\n, including the Dev Plan\n\n\nSyslog\n\n\nErrands\n\n\nResource Config\n\n\nStemcell\n\n\n\n\n Assign Availability Zones and Networks\n\n\nTo select AZs and networks for VMs used by PCC, do the following:\n\n\n\n\n\n\nClick \nAssign AZs and Networks\n.\n\n\n\n\n\n\nConfigure the fields on the \nAssign AZs and Networks\n pane as follows:\n   \n\n    \nField\nInstructions\n\n    \nPlace singleton jobs in\n\n        \nSelect the region that you want for singleton VMs.\n\n    \nBalance other jobs in\n\n        \nSelect the AZ(s) you want to use for distributing other GemFire VMs.\n            Pivotal recommends selecting all of them.\n\n    \nNetwork\n\n        \nSelect your PAS (or Elastic Runtime) network.\n\n    \nService Network\n\n        \nSelect the network to be used for GemFire VMs.\n \n  \n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\n Settings: Smoke Tests\n\n\nThe smoke-tests errand that runs after tile installation.\nThe errand verifies that your installation was successful.\nBy default, the \nsmoke-test\n errand runs on the \nsystem\n org and the \np-cloudcache-smoke-test\n space.\n\n\nNote\n: Smoke tests will fail unless you enable global default application security groups (ASGs). You can enable global default ASGs by binding the ASG to the \nsystem\n org without specifying a space. To enable global default ASGs, use \ncf bind-running-security-group\n.\n\n\n\nTo select which plan you want to use for smoke tests, do the following:\n\n\n\n\n\n\nClick \nSettings\n.\n\n\n\n\n\n\nSelect a plan to use when the \nsmoke-tests\n errand runs.\n\n\nEnsure the selected plan is enabled and configured.\n For information about configuring plans, see \nConfigure Service Plans\n below.\n If the selected plan is not enabled, the \nsmoke-tests\n errand fails.\n\n\nPivotal recommends that you use the smallest four-server plan for smoke tests.\n Because smoke tests create and later destroy this plan, using a very small plan reduces installation time.\n \n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nSettings: Allow Outbound Internet Access\n\n\nBy default, outbound internet access is not allowed from service instances. \n\n\nIf BOSH is configured to use an external blob store, you need allow outbound internet access from service instances.\nLog forwarding and backups, which require external endpoints, might also require internet access.\n\n\nTo allow outbound internet access from service instance, do the following:\n\n\n\n\n\n\nClick \nSettings\n.\n\n\n\n\n\n\nSelect \nAllow outbound internet access from service instances (IaaS-dependent)\n.\n\n\n\n\nNote\n: Outbound network traffic rules also depend on your IaaS settings. \n  Consult your network or IaaS administrator to ensure that your IaaS allows outbound traffic to the external networks you need.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\n Syslog\n\n\nBy default, syslog forwarding is not enabled in PCC.\nHowever, PCC supports forwarding syslog to an external log management service (for example, Papertrail, Splunk, or your custom enterprise log sink).\nThe broker logs are useful for debugging problems creating, updating, and binding service instances.\n\n\nTo enable remote syslog for the service broker, do the following:\n\n\n\n\nClick \nSyslog\n.\n   \n\n\n\n\nConfigure the fields on the \nSyslog\n pane as follows:\n   \n\n    \nField\nInstructions\n\n    \nEnable Remote Syslog\n\n        \nSelect to enable.\n\n    \nExternal Syslog Address\n\n        \nEnter the address or host of the syslog server for sending logs, for example, \nlogs.example.com\n.\n\n    \nExternal Syslog Port\n\n        \nEnter the port of the syslog server for sending logs, for example,\n29279\n.\n\n    \nEnable TLS for Syslog\n\n    \nSelect to enable secure log transmission through TLS. Without this, remote syslog sends unencrypted logs. We recommend enabling TLS, as most syslog endpoints such as Papertrail and Logsearch require TLS.\n\n    \nPermitted Peer for TLS Communication. This is required if TLS is enabled.\n\n        \nIf there are several peer servers that can respond to remote syslog connections, then\n            provide a regex, such as \n*.example.com\n.\n\n    \nCA Certificate for TLS Communication\n\n        \nIf the server certificate is not signed by a known authority, for example, an internal syslog server,\n            provide the CA certificate of the log management service endpoint.\n\n    \nSend service instance logs to external\n\n        \nBy default, only the broker logs are forwarded to your configured log management service.\n            If you want to forward server and locator logs from all service instances, select this.\n            This lets you monitor the health of the clusters, although it generates a large volume of logs.\n\n            If you don't enable this, you get only the broker logs which include information about service instance creation, \n            but not about on-going cluster health.\n\n  \n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\n\n\nConfigure Service Plans\n\n\nYou can configure five individual plans for your developers. Select the \nPlan 1\n through \nPlan 5\n tabs to configure each of them.\n\n\n\n\nThe \nPlan Enabled\n option is selected by default. If you do not want to add this plan to the CF service catalog, select \nPlan Disabled\n. You must enable at least one plan.\n\n\nThe \nPlan Name\n text field allows you to customize the name of the plan. This plan name is displayed to developers when they view the service in the Marketplace.\n\n\nThe \nPlan Description\n text field allows you to supply a plan description. The description is displayed to developers when they view the service in the Marketplace.\n\n\nThe \nEnable metrics for service instances\n checkbox enables metrics for service instances created using the plan.\nOnce enabled, the metrics are sent to the Loggregator Firehose.\n\n\nThe \nCF Service Access\n drop-down menu gives you the option to display or not display the service plan in the Marketplace.\n\nEnable Service Access\n displays the service plan the Marketplace.\n\nDisable Service Access\n makes the plan unavailable in the Marketplace. If you choose this option, you cannot make the plan available at a later time. \n\nLeave Service Access Unchanged\n makes the plan unavailable in the Marketplace by default, but allows you to make it available at a later time.\n\n\nThe \nService Instance Quota\n sets the maximum number of PCC clusters that can exist simultaneously.\n\n\nWhen developers create or update a service instance, they can specify the number of servers in the cluster. The \nMaximum servers per cluster\n field allows operators to set an upper bound on the number of servers developers can request. If developers do not explicitly specify the number of servers in a service instance, a new cluster has the number of servers specified in the \nDefault Number of Servers\n field.\n\n\nThe \nAvailability zones for service instances\n setting determines which AZs are used for a particular cluster. The members of a cluster are distributed evenly across AZs.\n\n\n WARNING!\n After you've selected AZs for your service network, you cannot add additional AZs; doing so causes existing service instances to lose data on update. \n\n\n\nThe remaining fields control the VM type and persistent disk type for servers and locators. The total size of the cache is directly related to the number of servers and the amount of memory of the selected server VM type. We recommend the following configuration:\n\n\n\n\nFor the \nVM type for the Locator VMs\n field, select a VM that has at least 2 CPUs, 1\nGB of RAM and 4\nGB of disk space.\n\n\nFor the \nPersistent disk type for the Locator VMs\n field, select 10\nGB or higher.\n\n\nFor the \nVM type for the Server VMs\n field, select a VM that has at least 2 CPUs, 4\nGB of RAM and 8\nGB of disk space.\n\n\nFor the \nPersistent disk type for the server VMs\n field, select 10 GB or higher.\n\n\n\n\nWhen you finish configuring the plan, click \nSave\n to save your configuration options.\n\n\n Configure a Dev Plan\n\n\nA Dev Plan is a type of service plan.\nUse a Dev Plan for development and testing.\nThe plan provides a single locator and server,\nwhich are colocated within a single VM. \n\n\nThe page for configuring a Dev Plan is similar to the page for configuring \nother service plans.\nTo configure the Dev Plan, input information in the fields and make selections from the options on the \nPlan for test development\n page. \n\n\n\n\nIf you have enabled post-deploy scripts in your BOSH Director,\na region is automatically created.\nTo confirm that post-deploy scripts are enabled, navigate to the \nDirector Config\n pane of Ops Manger Director and\nverify that \nEnable Post Deploy Scripts\n is selected.\n\n\n\n\n Errands\n\n\nBy default, post-deploy and pre-delete errands always run. \nPivotal recommends keeping these defaults.\nHowever, if necessary, you can change these defaults as follows.\n\n\nFor general information about errands in PCF, see \nManaging Errands in Ops Manager\n\n\n\n\n\n\nClick \nErrands\n.\n\n\n\n\n\n\nChange the setting for the errands.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\n Stemcell\n\n\nEnsure you import the correct type of stemcell indicated on this tab.\n\n\nYou can download the latest available stemcells from the \nPivotal Network\n.\n\n\nPCC 1.4 can use either v2.1.x or v2.0.x of \nPivotal Application Service (PAS)\nand PCF Operations Manager (Ops Manager).\nAs of v2.1.0, manage stemcells in the Ops Manager dashboard.\nStemcells do not appear in this tile configuration.\n\n\n Setting Service Instance Quotas\n\n\n<\n%= partial '../../p-cloud-cache/odb/set_quotas' %>\n\n\n Monitoring Pivotal Cloud Cache Service Instances\n\n\nPCC clusters and brokers emit service metrics. \nYou can use any tool that has a corresponding Cloud Foundry nozzle to read and monitor these metrics in real time.\n\n\nIn the descriptions of the metrics,\nKPI stands for Key Performance Indicator.\n\n\n Service Instance Metrics\n\n\n Member Count\n\n\n\n   \n \nserviceinstance.MemberCount\n\n   \n\n      \nDescription\n\n      \nReturns the number of members in the distributed system.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \nless than the manifest member count\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nThis depends on the expected member count, which is available in the BOSH manifest. If the number expected is different from the number emitted, this is a critical situation that may lead to data loss, and the reasons for node failure should be investigated by examining the service logs.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nMember loss due to any reason can potentially cause data loss.\n\n   \n\n\n\n\n\n Total Available Heap Size\n\n\n\n   \n \nserviceinstance.TotalHeapSize\n\n   \n\n      \nDescription\n\n      \nReturns the total available heap, in megabytes, across all instance members.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \npulse\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.\n\n   \n\n\n\n\n\n Total Used Heap Size\n\n\n\n   \n \nserviceinstance.UsedHeapSize\n\n   \n\n      \nDescription\n\n      \nReturns the total heap used across all instance members, in megabytes.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \npulse\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.\n\n   \n\n\n\n\n\n Total Available Heap Size as a Percentage\n\n\n\n   \n \nserviceinstance.UnusedHeapSizePercentage\n\n   \n\n      \nDescription\n\n      \nReturns the proportion of total available heap across all instance members, expressed as a percentage.\n\n   \n\n   \n\n      \nMetric Type\n\n      \npercent\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncompount metric\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \n40%\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \n10%\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nIf this is a spike due to eviction catching up with insert frequency, then customers need to keep a close watch that it should not hit the RED marker. If there is no eviction, then horizontal scaling is suggested.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the total heap size and used heap size are too close, the system might see thrashing due to GC activity. This increases latency.\n\n   \n\n\n\n\n\n Per Member Metrics\n\n\n Memory Used as a Percentage\n\n\n\n   \n \nmember.UsedMemoryPercentage\n\n   \n\n      \nDescription\n\n      \nRAM being consumed.\n\n   \n\n   \n\n      \nMetric Type\n\n      \npercent\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nAverage over last 10 minutes\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \naverage\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \n75%\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \n85%\n\n   \n\n\n\n\n\n Count of Java Garbage Collections\n\n\n\n   \n \nmember.GarbageCollectionCount\n\n   \n\n      \nDescription\n\n      \nThe number of times that garbage has been collected.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nSum over last 10 minutes\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nCheck the number of queries run against the system, which increases the deserialization of objects and increases garbage.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the frequency of garbage collection is high, the system might see high CPU usage, which causes delays in the cluster.\n\n   \n\n\n\n\n\n CPU Utilization Percentage\n\n\n\n   \n \nmember.HostCpuUsage\n\n   \n\n      \nDescription\n\n      \nThis member's process CPU utilization, expressed as a percentage.\n\n   \n\n   \n\n      \nMetric Type\n\n      \npercent\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nAverage over last 10 minutes\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \naverage\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \n85%\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \n95%\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nIf this is not happening with high GC activity, the system is reaching its limits. Horizontal scaling might help.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nHigh CPU usage causes delayed responses and can also make the member non-responsive. This can cause the member to be kicked out of the cluster, potentially leading to data loss.\n\n   \n\n\n\n\n\n Average Latency of Get Operations\n\n\n\n   \n \nmember.GetsAvgLatency\n\n   \n\n      \nDescription\n\n      \nThe average latency of cache get operations, in nanoseconds.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nAverage over last 10 minutes\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \naverage\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nIf this is not happening with high GC activity, the system is reaching its limit. Horizontal scaling might help.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIt is a good indicator of the overall responsiveness of the system. If this number is high, the service administrator should diagnose the root cause.\n\n   \n\n\n\n\n\n Average Latency of Put Operations\n\n\n\n   \n \nmember.PutsAvgLatency\n\n   \n\n      \nDescription\n\n      \nThe average latency of cache put operations, in nanoseconds.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nAverage over last 10 minutes\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \naverage\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nIf this is not happening with high GC activity, the system is reaching its limit. Horizontal scaling might help.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIt is a good indicator of the overall responsiveness of the system. If this number is high, the service administrator should diagnose the root cause.\n\n   \n\n\n\n\n\n JVM pauses\n\n\n\n   \n \nmember.JVMPauses\n\n   \n\n      \nDescription\n\n      \nThe quantity of JVM pauses.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nSum over 2 seconds\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \nDependent on the IaaS and app use case.\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nCheck the cached object size; if it is greater than 1 MB, you may be hitting the limitation on JVM to garbage collect this object. Otherwise, you may be hitting the utilization limit on the cluster, and will need to scale up to add more memory to the cluster.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nDue to a JVM pause, the member stops responding to \"are-you-alive\" messages, which may cause this member to be kicked out of the cluster.\n\n   \n\n\n\n\n\n File Descriptor Limit\n\n\n\n   \n \nmember.FileDescriptorLimit\n\n   \n\n      \nDescription\n\n      \nThe maximum number of open file descriptors allowed for the member\u2019s host operating system.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \npulse\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.\n\n   \n\n\n\n\n\n Open File Descriptors\n\n\n\n   \n \nmember.TotalFileDescriptorOpen\n\n   \n\n      \nDescription\n\n      \nThe current number of open file descriptors.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \npulse\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.\n\n   \n\n\n\n\n\n Quantity of Remaining File Descriptors\n\n\n\n   \n \nmember.FileDescriptorRemaining\n\n   \n\n      \nDescription\n\n      \nThe number of available file descriptors.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nSuggested measurement\n\n      \nEvery second\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncompound metric\n\n   \n\n   \n\n      \nWarning Threshold\n\n      \n1000\n\n   \n\n   \n\n      \nCritical Threshold\n\n      \n100\n\n   \n\n   \n\n      \nSuggested Actions\n\n      \nScale horizontally to increase capacity.\n\n   \n\n   \n\n      \nWhy a KPI?\n\n      \nIf the number of open file descriptors exceeds number available, it causes the member to stop responding and crash.\n\n   \n\n\n\n\n\n Gateway Sender and Gateway Receiver Metrics\n\n\nThese are metrics emitted through the CF Nozzle for gateway senders and gateway receivers.\n\n\n Queue Size for the Gateway Sender\n\n\n\n   \ngatewaySender.\nsender-id\n.EventQueueSize\n \n\n   \n\n      \nDescription\n\n      \nThe current size of the gateway sender queue. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Events Received at the Gateway Sender\n\n\n\n   \ngatewaySender.\nsender-id\n.EventsReceivedRate\n \n\n   \n\n      \nDescription\n\n      \nA count of the events coming from the region to which the gateway sender is attached. It is the count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway sender was created. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Events Queued by the Gateway Sender\n\n\n\n   \ngatewaySender.\nsender-id\n.EventsQueuedRate\n \n\n   \n\n      \nDescription\n\n      \nA count of the events queued on the gateway sender from the region. This quantity of events might be lower than the quantity of events received, as not all received events are queued. It is a count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway sender was created.\n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Events Received by the Gateway Receiver\n\n\n\n   \ngatewayReceiver.EventsReceivedRate\n \n\n   \n\n      \nDescription\n\n      \nA count of the events received from the gateway sender which will be applied to the region on the gateway receiver's site. It is the count since the last time the metric was checked. The first time it is checked, the count is of the number of events since the gateway receiver was created. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Disk Metrics\n\n\nThese are metrics emitted through the CF Nozzle for disks.\n\n\n Average Latency of Disk Writes\n\n\n\n   \ndiskstore.DiskWritesAvgLatency\n \n\n   \n\n      \nDescription\n\n      \nThe average latency of disk writes in nanoseconds. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ntime in nanoseconds\n\n   \n\n\n\n\n\n Quantity of Bytes on Disk\n\n\n\n   \ndiskstore.TotalSpace\n \n\n   \n\n      \nDescription\n\n      \nThe total number of bytes on the attached disk. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Quantity of Available Bytes on Disk\n\n\n\n   \ndiskstore.UseableSpace\n \n\n   \n\n      \nDescription\n\n      \nThe total number of bytes of available space on the attached disk. \n\n   \n\n   \n\n      \nMetric Type\n\n      \nnumber\n\n   \n\n   \n\n      \nMeasurement Type\n\n      \ncount\n\n   \n\n\n\n\n\n Total Memory Consumption\n\n\nThe BOSH \nmem-check\n errand calculates and outputs\nthe quantity of memory used across all PCC service instances.\nThis errand helps PCF operators monitor resource costs,\nwhich are based on memory usage.\n\n\nFrom the director, run a bosh command of the form:\n\n\nbosh -d \nservice broker name\n run-errand mem-check\n\n\n\n\nWith this command:\n\n\nbosh -d cloudcache-service-broker run-errand mem-check\n\n\n\n\nHere is an anonymized portion of example output from the \nmem-check\n errand\nfor a two cluster deployment:\n\n\n           Analyzing deployment xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx1...\n           JVM heap usage for service instance xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx1\n           Used Total = 1204 MB\n           Max Total = 3201 MB\n\n           Analyzing deployment xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2...\n           JVM heap usage for service instance xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2\n           Used Total = 986 MB\n           Max Total = 3201 MB\n\n           JVM heap usage for all clusters everywhere:\n           Used Global Total = 2390 MB\n           Max Global Total = 6402 MB\n\n\n\n\n Monitoring PCC Service Instances with Prometheus\n\n\nPrometheus is one of various tools you can use to monitor services instances. It is a monitoring and alerting toolkit that allows for metric scraping.\nYou can use the \nFirehose exporter\n to export all the metrics from the Firehose,\nwhich you can then graph with \nGrafana\n to monitor your PCC cluster.\n\n\nFollow the instructions \nhere\n to deploy Prometheus alongside your PCF cluster.\n\n\nPrometheus can be deployed on any IaaS.\nYou need to verify that the Firehose exporter job can talk to your UAA VM.\nThis might involve opening up firewall rules or enabling your VM to allow outgoing traffic.\n\n\n\n\nYou can run queries on, and build a custom dashboard of, specific metrics that are important to you.\n\n\n Upgrading Pivotal Cloud Cache\n\n\nFollow the steps below to upgrade PCC:\n\n\n\n\nDownload the new version of the tile from the Pivotal Network.\n\n\nUpload the product to Ops Manager.\n\n\nClick \nAdd\n next to the uploaded product.\n\n\nClick on the Cloud Cache tile and review your configuration options.\n\n\nClick \nApply Changes\n.\n\n\n\n\n Updating Pivotal Cloud Cache Plans\n\n\nFollow the steps below to update plans in Ops Manager.\n\n\n\n\nClick on the Cloud Cache tile.\n\n\nClick on the plan you want to update under the \nInformation\n section.\n\n\nEdit the fields with the changes you want to make to the plan.\n\n\nClick \nSave\n button on the bottom of the page.\n\n\nClick on the \nPCF Ops Manager\n to navigate to the \nInstallation Dashboard\n.\n\n\nClick \nApply Changes\n.\n\n\n\n\nPlan changes are not applied to existing services instances until you run the \nupgrade-all-service-instances\n BOSH errand. You must use the BOSH CLI to run this errand. Until you run this errand, developers cannot update service instances.\n\n\nChanges to fields that can be overridden by optional parameters, for example  \nnum_servers\n or \nnew_size_percentage\n, change the default value of these instance properties, but do not affect existing service instances. \n\n\nIf you change the allowed limits of an optional parameter, for example the maximum number of servers per cluster, existing service instances in violation of the new limits are not modified.\n\n\nWhen existing instances are upgraded, all plan changes are applied to them.\n\n\n Uninstalling Pivotal Cloud Cache\n\n\nTo uninstall PCC, follow the steps from below from the \nInstallation Dashboard\n:\n\n\n\n\nClick the trash can icon in the bottom-right-hand corner of the tile.\n\n\nClick \nApply Changes\n.\n\n\n\n\n Troubleshooting\n\n\n View Statistics Files\n\n\nYou can visualize the performance of your cluster by downloading the statistics files from your servers.\nThese files are located in the persistent store on each VM.\nTo copy these files to your workstation, run the following command:\n\n\n`bosh2 -e BOSH-ENVIRONMENT -d DEPLOYMENT-NAME scp server/0:/var/vcap/store/gemfire-server/statistics.gfs /tmp`\n\n\n\n\n\nSee the Pivotal GemFire \nInstalling and Running VSD\n topic for information about loading the statistics files into Pivotal GemFire VSD.\n\n\n Smoke Test Failures\n\n\nError: \"Creating p-cloudcache SERVICE-NAME failed\"\n\n\nThe smoke tests could not create an instance of GemFire. To troubleshoot\nwhy the deployment failed, use the cf CLI to create a new service instance using the same plan and download the logs of the service deployment from BOSH.\n\n\nError: \"Deleting SERVICE-NAME failed\"\n\n\nThe smoke test attempted to clean up a service instance it created and failed to delete the service using the \ncf delete-service\n command. To troubleshoot this issue, run BOSH \nlogs\n to view the logs on the broker or the service instance to see why the deletion may have failed.\n\n\nError: Cannot connect to the cluster SERVICE-NAME\n\n\nThe smoke test was unable to connect to the cluster.\n\n\nTo troubleshoot the issue, review the logs of your load balancer, and\nreview the logs of your CF Router to ensure the route to your PCC\ncluster is properly registered.\n\n\nYou also can create a service instance and try to connect to it using the\ngfsh CLI. This requires creating a service key.\n\n\nError: \"Could not perform create/put on Cloud Cache cluster\"\n\n\nThe smoke test was unable to write data to the cluster. The user may not have\npermissions to create a region or write data.\n\n\nError: \"Could not retrieve value from Cloud Cache cluster\"\n\n\nThe smoke test was unable to read back the data it wrote. Data loss can happen\nif a cluster member improperly stops and starts again or if the member machine\ncrashes and is resurrected by BOSH. Run BOSH \nlogs\n to view the logs on the\nbroker to see if there were any interruptions to the cluster by a service\nupdate.\n\n\n General Connectivity\n\n\nClient-to-Server Communication\n\n\nPCC Clients communicate to PCC servers on port 40404 and with\nlocators on port 55221.\nBoth of these ports must be reachable from the PAS (or Elastic Runtime) network to service the network.\n\n\nMembership Port Range\n\n\nPCC servers and locators communicate with each other using UDP and TCP. The current port range for this communication is \n49152-65535\n.\n\n\nIf you have a firewall between VMs, ensure this port range is open.", 
            "title": "Operator"
        }, 
        {
            "location": "/operator/#service-network", 
            "text": "You must have access to a Service Network in order to install PCC.", 
            "title": "Service Network"
        }, 
        {
            "location": "/operator/#settings-allow-outbound-internet-access", 
            "text": "By default, outbound internet access is not allowed from service instances.   If BOSH is configured to use an external blob store, you need allow outbound internet access from service instances.\nLog forwarding and backups, which require external endpoints, might also require internet access.  To allow outbound internet access from service instance, do the following:    Click  Settings .    Select  Allow outbound internet access from service instances (IaaS-dependent) .   Note : Outbound network traffic rules also depend on your IaaS settings. \n  Consult your network or IaaS administrator to ensure that your IaaS allows outbound traffic to the external networks you need.    Click  Save .", 
            "title": "Settings: Allow Outbound Internet Access"
        }, 
        {
            "location": "/operator/#configure-service-plans", 
            "text": "You can configure five individual plans for your developers. Select the  Plan 1  through  Plan 5  tabs to configure each of them.   The  Plan Enabled  option is selected by default. If you do not want to add this plan to the CF service catalog, select  Plan Disabled . You must enable at least one plan.  The  Plan Name  text field allows you to customize the name of the plan. This plan name is displayed to developers when they view the service in the Marketplace.  The  Plan Description  text field allows you to supply a plan description. The description is displayed to developers when they view the service in the Marketplace.  The  Enable metrics for service instances  checkbox enables metrics for service instances created using the plan.\nOnce enabled, the metrics are sent to the Loggregator Firehose.  The  CF Service Access  drop-down menu gives you the option to display or not display the service plan in the Marketplace. Enable Service Access  displays the service plan the Marketplace. Disable Service Access  makes the plan unavailable in the Marketplace. If you choose this option, you cannot make the plan available at a later time.  Leave Service Access Unchanged  makes the plan unavailable in the Marketplace by default, but allows you to make it available at a later time.  The  Service Instance Quota  sets the maximum number of PCC clusters that can exist simultaneously.  When developers create or update a service instance, they can specify the number of servers in the cluster. The  Maximum servers per cluster  field allows operators to set an upper bound on the number of servers developers can request. If developers do not explicitly specify the number of servers in a service instance, a new cluster has the number of servers specified in the  Default Number of Servers  field.  The  Availability zones for service instances  setting determines which AZs are used for a particular cluster. The members of a cluster are distributed evenly across AZs.   WARNING!  After you've selected AZs for your service network, you cannot add additional AZs; doing so causes existing service instances to lose data on update.   The remaining fields control the VM type and persistent disk type for servers and locators. The total size of the cache is directly related to the number of servers and the amount of memory of the selected server VM type. We recommend the following configuration:   For the  VM type for the Locator VMs  field, select a VM that has at least 2 CPUs, 1 GB of RAM and 4 GB of disk space.  For the  Persistent disk type for the Locator VMs  field, select 10 GB or higher.  For the  VM type for the Server VMs  field, select a VM that has at least 2 CPUs, 4 GB of RAM and 8 GB of disk space.  For the  Persistent disk type for the server VMs  field, select 10 GB or higher.   When you finish configuring the plan, click  Save  to save your configuration options.", 
            "title": "Configure Service Plans"
        }, 
        {
            "location": "/operator/#error-creating-p-cloudcache-service-name-failed", 
            "text": "The smoke tests could not create an instance of GemFire. To troubleshoot\nwhy the deployment failed, use the cf CLI to create a new service instance using the same plan and download the logs of the service deployment from BOSH.", 
            "title": "Error: \"Creating p-cloudcache SERVICE-NAME failed\""
        }, 
        {
            "location": "/operator/#error-deleting-service-name-failed", 
            "text": "The smoke test attempted to clean up a service instance it created and failed to delete the service using the  cf delete-service  command. To troubleshoot this issue, run BOSH  logs  to view the logs on the broker or the service instance to see why the deletion may have failed.", 
            "title": "Error: \"Deleting SERVICE-NAME failed\""
        }, 
        {
            "location": "/operator/#error-cannot-connect-to-the-cluster-service-name", 
            "text": "The smoke test was unable to connect to the cluster.  To troubleshoot the issue, review the logs of your load balancer, and\nreview the logs of your CF Router to ensure the route to your PCC\ncluster is properly registered.  You also can create a service instance and try to connect to it using the\ngfsh CLI. This requires creating a service key.", 
            "title": "Error: Cannot connect to the cluster SERVICE-NAME"
        }, 
        {
            "location": "/operator/#error-could-not-perform-createput-on-cloud-cache-cluster", 
            "text": "The smoke test was unable to write data to the cluster. The user may not have\npermissions to create a region or write data.", 
            "title": "Error: \"Could not perform create/put on Cloud Cache cluster\""
        }, 
        {
            "location": "/operator/#error-could-not-retrieve-value-from-cloud-cache-cluster", 
            "text": "The smoke test was unable to read back the data it wrote. Data loss can happen\nif a cluster member improperly stops and starts again or if the member machine\ncrashes and is resurrected by BOSH. Run BOSH  logs  to view the logs on the\nbroker to see if there were any interruptions to the cluster by a service\nupdate.", 
            "title": "Error: \"Could not retrieve value from Cloud Cache cluster\""
        }, 
        {
            "location": "/operator/#client-to-server-communication", 
            "text": "PCC Clients communicate to PCC servers on port 40404 and with\nlocators on port 55221.\nBoth of these ports must be reachable from the PAS (or Elastic Runtime) network to service the network.", 
            "title": "Client-to-Server Communication"
        }, 
        {
            "location": "/operator/#membership-port-range", 
            "text": "PCC servers and locators communicate with each other using UDP and TCP. The current port range for this communication is  49152-65535 .  If you have a firewall between VMs, ensure this port range is open.", 
            "title": "Membership Port Range"
        }, 
        {
            "location": "/release-notes/", 
            "text": "v1.4.0\n\n\nRelease Date:\n  May 14, 2018\n\n\nFeatures included in this release:\n\n\n\n\nPCC is now running Pivotal Gemfire 9.3.0.\n\n\nPCC now supports connecting more than two clusters via WAN.\nThis facilitates implementing design patterns such as Hub-and-Spoke.\n\n\nThe new \nmem-check\n BOSH errand generates a report containing\nthe current data consumption by PCC service instances and total JVM sizes.\n\n\nPCC now supports Pivotal Application Service (PAS) 2.1.\n\n\nPCC selects minimum default values for VM types on PAS 2.1 and 2.0.\n\n\n\n\nKnown issues in this release:\n\n\n\n\n\n\nIf you upgrade to PCC v1.3 as part of the process of upgrading to this\n1.4 release, and you created service keys on PCC before you installed v1.3:\ndelete and recreate the service keys so that users are properly\nassigned roles for authentication and authorization within the cluster.\nThen, rebind all your apps.\nFor information about how to perform these tasks,\nsee \nDelete a Service Key\n,\n\nCreate Service Keys\n,\nand \nBind an App to a Service Instance\n.\n\n\n\n\n\n\nA Pulse topology diagram might not be accurate and might show more members than are actually in the cluster. However, the numerical value displayed on the top bar is accurate.\n\n\n\n\n\n\nRelease Notes for Earlier Versions\n\n\nFor v1.3.x versions of PCC,\nsee \nRelease Notes\n in the v1.3 version of this documentation.\n\n\nFor v1.2.x versions of PCC,\nsee \nRelease Notes\n in the v1.2 version of this documentation.\n\n\nFor v1.1.x versions of PCC,\nsee \nRelease Notes\n in the v1.1 version of this documentation.\n\n\nFor v1.0.x versions of PCC,\nsee \nRelease Notes\n in the v1.0 version of this documentation.", 
            "title": "Release Notes"
        }
    ]
}